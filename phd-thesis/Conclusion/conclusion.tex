\chapter{SUMMARY AND FUTURE WORK}
\section{SUMMARY}
\noindent This work has investigated the role of galaxy interaction with respect to galaxy evolution. To fully constrain the link between them, large samples of interacting galaxies must be obtained. In chapter 2, a large catalogue of interacting galaxies was created. This utilised novel techniques, with newly developed data-access architecture to classify an unprecedented number of single-band galactic images into interacting and non-interacting systems. This resulted in a sample of 21,926 tidally disturbed and interacting systems, the largest sample classified morphologically to date. To demonstrate it's use, this sample was cross-matched to existing catalogues of galaxies with ancillary data. The largest overlap between the two was with the COSMOS field, with a sample of 4,135 interacting galaxies to explore.

Chapter 3 demonstrated this process, and the link between interaction stage and multiple physical processes were examined. It was found that the star formation rate in interacting galaxies is significantly enhanced as the stage increases towards a merger. This mirrored existing works which often used the projected separation of two systems as an approximation of the stage of interaction in the two systems. However, we find a complete disconnect with the projected separation and stage, noting that conflicting results can emerge from only looking at the projected separation without properly accounting for the interaction stage from the morphology. This was particularily true of the change in the fraction of active galactic nuclei with stage, a topic the literature is particularily divided upon. Two modes of activation were found, one at stage 2 and one at stage 3. Thus, this was not only evidence for AGN flickering, but also that there is some delay in the activation of the AGN which is difficult to account for when only using projected separation. While this chapter linked the underlying physical processes in interaction to the interaction stage, we also aimed to link the formation of tidal features and change in physical processes with underlying parameters of interaction.

Chapter 4 saw the development of an algorithm to achieve just this. By combining a fast, efficient restricted numerical simulation with a markov-chain monte carlo methodology and Bayesian statistics, we are able to put constraints on the underlying parameters of 38 simulated interacting systems. While our uncertainties increased when applied to observational data, constraints were nonetheless able to be made upon them. The creation of this algorithm, APySPAM, paves the way for being able to apply this to large interacting galaxy datasets like the one created in chapter 2. However, the current limitation exists in the runtime of the algorithm; taking approximately 15-20 hours per interacting galaxy. Thus, methods of further improvement in efficiency of the underling numerical simulation must be explored. There is particular promise in this regard with the development of numerical simulations on GPUs, and the massive acceleration they provide to such projects.

Thus, I have created a large interacting galaxy dataset and demonstrated it's capability in the context of how the progression of a galaxy changes the underlying properties of galaxies. We have introduced a method by which further examination of these systems could be explored, although further advances in its efficiency must be made for this approach to be viable. Potential methods, plus descriptions of future works in chapter 2 and 3 are discussed in the final subsection of this thesis.

\section{FUTURE WORK}
\subsection{Catalogues of Galaxy Morphology}
% Creating the largest catalogue of interacting galaxies has been done. Why not for all morphologies? Demonstrates the capability of ESA Datalabs. Mention this combination with Euclid, JWST and such.
\noindent In chapter 2, it was demonstrated how new data access architecture is now at a point where millions of sources across multiple filters, instruments and observatories can be classified to yield unprecedented sample sizes. These can be combined with novel machine learning algorithms requiring small training set sizes to fine-tune, and thus being versitile and accurate across many different observatories. Since the publication of the paper underlaying chapter 2, the \texttt{Zoobot} algorithm has been updated many times. Not only is it now trained upon \textit{HST} data, but part of the representations of galaxies it learns is if a galaxy is interacting or not. Thus, by redoing the work conducted in chapter 2, one could classify galactic morphologies across the Galaxy Zoo workflow rather than just an individual question.

The fundamental component of this which makes it possible is the ESA: Datalabs platform. This platform has also been heaily updated. Now, it not only has many different observatory's archives within it (such as JWST and Euclid) but also has access to pipeline creation and its own GPUs. Thus, the entire work of chapter 2 can be done on the platform with no requirement for data transfer. Thus, the age of colossal sample sizes as classified by machine learning algorithms is just beginning. These will be from the three major observatories currently in operation across the world, with the Legacy Survey of Space and Time (LSST) consortium building its own platform.

The next step will be to create the largest catalogue of galaxy morphology across all the data within ESA Datalabs. With the pipelines created in this work and the ongoing development of machine learning algorithms like \texttt{Zoobot}, these samples will be made with relative ease.

\subsection{Staging of Interaction}
% This has been just one use of the OR23 catalogue. Describe other things that can be done, overlap with other catalogues, deeper look at interacting galaxies using existing pipelines for reducing photometry. Perhaps mention MOONS and other survey scale telescopes.
\noindent The use of such a large catalogue of galaxy morphology has been demonstrated in chapter 3. However, this only used the data from a single catalogue: the COSMOS catalogue. This survey contains nearly 2 million objects, but is only over a relatively small patch of the sky: a 2$\times$2 square degree area. Thus, we require more ancillary data to make further judgements from the catalogues created here, but also in future. There are many other catalogues that can be of use, such as the WISE catalogue, SDSS, etc. However, the real change will be the use of Euclid here. Euclid is an all-sky survey that will produce broadband photometry and spectroscopy for millions of different sources. With such added data to go with our large morphology catalogues, work like that done in chapter 2 will be even more insightful and statistically robust.

Two ground based observatories that will also heavily impact these new samples will be surveys like LSST and the Multi-Object Optical and Near-Infrared Spectrograph (MOONS). These surveys will not only provide us full sky coverage (in their respective hemispheres), but MOONS in particular will provide us with spectroscopy of the highest redshift galaxies. With this increased redshift information and coverage, whether photometrically or spectroscopically, we will be better able to constrain the relationship between galaxy interaction and evolution by removing contamination in our samples. 

The question relating interaction and stage can be expanded outwith interaction entirely. With large catalogues of galaxies with morphology classification and photometric and redshift information, relations between stellar mass, environment, star formation rate and AGN fraction will be accurately matched to morphology. 

\subsection{Constraining Interacting Galaxy Parameters}
% ALOT to talk about here. So, can discuss how GPUs are beginning to run numerical simulations with unprecedented efficiency. Talk about the rise in simulation based inference, and the easy transference of this pipeline to that. 
\noindent Finally, there is the question of relating the underlying parameters of interaction to the tidal features that form. In chapter 4, we attempted to link these directly by mapping the flux distribution of interacting galaxies to that of simulated interactions. In these, we were able to find constraints and reveal degeneracies across multiple parameters. However, the limitation of this approach was the computational expense, taking approximately 15 hours per system. For the large scale surveys and catalogues of the future, this is not feasible to apply. However, there are multiple development routes that this could go down. 

The first is to completely update the code to be accessible on a GPU. Development of numerical codes on GPUS remains, at time of writing, in its infancy. However, there have been striking results in improvements in efficiency up to a factor of 3 \citep{Paper of NVIDEA numerical simulation}. Thus, reworking the code in this way would remove this limitation and have constraint being made in a matter of minutes: much more feasible for applying to large scale samples.

A second approach would be to move away from the direct, brute force method and into utilising simulation based inference (SBI). Much work has been conducted into SBI, and its massive increases in efficiency when constraining over large and complex parameter spaces \citep{Papers by Nial Jeffery}. In this context, rather than running an MCMC and directly comparing simulation outputs to an observational image, a machine learning algorithm is used for the comparison. By running an initial set of simulations and feeding this into a rudimentary CNN, a low dimensional representative vector can be created of each image. This is then applied to the observational image. The CNN then approximates the posterior through parameter space, achieving excellent accuracy in the cited cases. However, while this approach has been very successful for the 1D case, and approximating different distributions work is still requried in image recognition - especially across the 14 dimensions of interacting parameter space presented here. 

In the context of large samples of interacting galaxies, SBI massively improves the efficiency of our constraining algorithms. Rather than running thousands of simulations on every single system to constrain it, we can front-load the computational expense of running the simulations to train the CNN and then apply the CNN to the entire interacting galaxy sample. This does have some caveats, however. The resolution of each image would have to be identical, and the redshift of the system would have to be accounted for. However, once further work SBI with images is conducted, it will become a valid approach. 