\chapter{SUMMARY AND FUTURE WORK}
\section{SUMMARY}
\noindent This work has investigated the role of galaxy interaction with respect to galaxy evolution, shown a new and novel way of creating large interacting galaxy samples and presented the initial pilot of a pipeline to definitively link the parameters of galaxies to the tidal features that form and their underlying processes. The novel process of creating a large catalogue of interacting galaxies was detailed in chapter 2. By utilising newly developed data-access architecture with the newly developed Bayesian CNN \texttt{Zoobot} the largest catalogue of interacting galaxies to date was created. Along with this, we demonstrated how the new data-access architecture, ESA Datalabs, can be used to explore archival observations like never before. To make concrete links between galaxy evolution and interaction in the local volume, this catalogue was cross-matched to catalogues of ancillary data. This was done using the catalogues created from deep observations of the COSMOS field. This provided us with a sample of 4,135 interacting galaxies to explore.

In Chapter 3 we conducted this exploration of our cross-matched sample in the context of interaction stage. We investigated whether the often conflicting theories about galaxy interaction were, in fact, from not accounting for interaction stage based on observed morphology. We confirmed that interaction stage does have a significant impact on the underlying processes and enhancements that result from interaction. It was immediately found that the SFR increases dramatically through stage - from close pair to merger. This was demonstrated by measuring the distributions of stellar mass and SFR through stage and comparing them. We found that for distributions of identical mass, the distribution of SFR changes completely. This is in the form of the disappearence of the red sequence, as our samples SFR was enhanced. We compared this with existing works which utilise the projected separation between systems as an approximation of the stage of interaction in the two systems. We find a complete disconnect with the projected separation and stage, noting that conflicting results can emerge from only looking at the projected separation without properly accounting for the interaction stage from the morphology. 

This degeneracy in the projected separation was particularily true of the change in the fraction of active galactic nuclei with stage, a topic the literature is particularily divided upon. Two modes of activation were found, one at stage 2 and one at stage 3. Thus, this was not only evidence for AGN flickering, but also that there is some delay in the activation of the AGN which is difficult to account for when only using projected separation. While this methodology shows how we can use ancillary data to connect the underlying parameters of galaxies to interaction, and fundamental processes we attempted to conduct this in a more general way and to bring in linking to tidal feature formation. 

Chapter 4 saw the development of an algorithm to find the underlying parameters the galaxys' involved in interaction purely from their flux distribution. We combine a fast, efficient restricted numerical simulation with a MCMC methodology and Bayesian statistics and put constraints on the underlying parameters of 38 simulated interacting systems. While our uncertainties increased when applied to observational data, constraints were nonetheless able to be made upon them. The creation of this algorithm, APySPAM, paves the way to apply this to large interacting galaxy datasets like the one created in chapter 2. However, the limitations in the runtime of the algorithm; taking approximately 15-20 hours per interacting galaxy, leads this to not be feasible. Thus, methods of further improvement in efficiency of the underling numerical simulation must be explored. There is particular promise in this regard with the development of numerical simulations on GPUs, and the massive acceleration they provide to such projects.

Thus, I have created a large interacting galaxy dataset and demonstrated it's capability in the context of how the progression of a galaxy changes the underlying properties of galaxies. We have introduced a method by which further examination of these systems could be explored, although further advances in its efficiency must be made for this approach to be viable. Potential methods, plus descriptions of future works in chapter 2 and 3 are discussed in the final subsection of this thesis.

\section{FUTURE WORK}
\subsection{Catalogues of Galaxy Morphology}
% Creating the largest catalogue of interacting galaxies has been done. Why not for all morphologies? Demonstrates the capability of ESA Datalabs. Mention this combination with Euclid, JWST and such.
\noindent In chapter 2, it was demonstrated how new data access architecture is now at a point where millions of sources across multiple filters, instruments and observatories can be classified to yield unprecedented sample sizes. These can be combined with novel machine learning algorithms requiring small training set sizes to fine-tune, and thus being versitile and accurate across many different observatories. Since the publication of the paper underlaying chapter 2, the \texttt{Zoobot} algorithm has been updated many times. Not only is it now trained upon \textit{HST} data, but part of the representations of galaxies it learns is if a galaxy is interacting or not. Thus, by redoing the work conducted in chapter 2, one could classify galactic morphologies across the Galaxy Zoo workflow rather than just an individual question.

The fundamental component of this which makes it possible is the ESA: Datalabs platform. This platform has also been heaily updated. Now, it not only has many different observatory's archives within it (such as JWST and Euclid) but also has access to pipeline creation and its own GPUs. Thus, the entire work of chapter 2 can be done on the platform with no requirement for data transfer. Thus, the age of colossal sample sizes as classified by machine learning algorithms is just beginning. These will be from the three major observatories currently in operation across the world, with the Legacy Survey of Space and Time (LSST) consortium building its own platform.

The next step will be to create the largest catalogue of galaxy morphology across all the data within ESA Datalabs. With the pipelines created in this work and the ongoing development of machine learning algorithms like \texttt{Zoobot}, these samples will be made with relative ease.

\subsection{Staging of Interaction}
% This has been just one use of the OR23 catalogue. Describe other things that can be done, overlap with other catalogues, deeper look at interacting galaxies using existing pipelines for reducing photometry. Perhaps mention MOONS and other survey scale telescopes.
\noindent The use of such a large catalogue of galaxy morphology has been demonstrated in chapter 3. However, this only used the data from a single catalogue: the COSMOS catalogue. This survey contains nearly 2 million objects, but is only over a relatively small patch of the sky: a 2$\times$2 square degree area. Thus, we require more ancillary data to make further judgements from the catalogues created here, but also in future. There are many other catalogues that can be of use, such as the WISE catalogue, SDSS, etc. However, the real change will be the use of Euclid here. Euclid is an all-sky survey that will produce broadband photometry and spectroscopy for millions of different sources. With such added data to go with our large morphology catalogues, work like that done in chapter 2 will be even more insightful and statistically robust.

Two ground based observatories that will also heavily impact these new samples will be surveys like LSST and the Multi-Object Optical and Near-Infrared Spectrograph (MOONS). These surveys will not only provide us full sky coverage (in their respective hemispheres), but MOONS in particular will provide us with spectroscopy of the highest redshift galaxies. With this increased redshift information and coverage, whether photometrically or spectroscopically, we will be better able to constrain the relationship between galaxy interaction and evolution by removing contamination in our samples. 

The question relating interaction and stage can be expanded outwith interaction entirely. With large catalogues of galaxies with morphology classification and photometric and redshift information, relations between stellar mass, environment, star formation rate and AGN fraction will be accurately matched to morphology. 

\subsection{Constraining Interacting Galaxy Parameters}
% ALOT to talk about here. So, can discuss how GPUs are beginning to run numerical simulations with unprecedented efficiency. Talk about the rise in simulation based inference, and the easy transference of this pipeline to that. 
\noindent Finally, there is the question of relating the underlying parameters of interaction to the tidal features that form. In chapter 4, we attempted to link these directly by mapping the flux distribution of interacting galaxies to that of simulated interactions. In these, we were able to find constraints and reveal degeneracies across multiple parameters. However, the limitation of this approach was the computational expense, taking approximately 15 hours per system. For the large scale surveys and catalogues of the future, this is not feasible to apply. However, there are multiple development routes that this could go down. 

The first is to completely update the code to be accessible on a GPU. Development of numerical codes on GPUS remains, at time of writing, in its infancy. However, there have been striking results in improvements in efficiency up to a factor of 3. Thus, reworking the code in this way would remove this limitation and have constraint being made in a matter of minutes: much more feasible for applying to large scale samples.

A second approach would be to move away from the direct, brute force method and into utilising simulation based inference (SBI). Much work has been conducted into SBI, and its massive increases in efficiency when constraining over large and complex parameter spaces \citep{Papers by Nial Jeffery}. In this context, rather than running an MCMC and directly comparing simulation outputs to an observational image, a machine learning algorithm is used for the comparison. By running an initial set of simulations and feeding this into a rudimentary CNN, a low dimensional representative vector can be created of each image. This is then applied to the observational image. The CNN then approximates the posterior through parameter space, achieving excellent accuracy in the cited cases. However, while this approach has been very successful for the 1D case, and approximating different distributions work is still requried in image recognition - especially across the 14 dimensions of interacting parameter space presented here. 

In the context of large samples of interacting galaxies, SBI massively improves the efficiency of our constraining algorithms. Rather than running thousands of simulations on every single system to constrain it, we can front-load the computational expense of running the simulations to train the CNN and then apply the CNN to the entire interacting galaxy sample. This does have some caveats, however. The resolution of each image would have to be identical, and the redshift of the system would have to be accounted for. However, once further work SBI with images is conducted, it will become a valid approach. 