\chapter{Summary and Future Work}\label{chapter:conclusion}
\section{Summary}
\noindent This work has focused on the role of galaxy interaction with respect to galaxy evolution. We have shown a new and novel way of creating large interacting galaxy samples, used it to explore the relation between galactic parameters and interaction and presented the initial pilot of a MCMC algorithm to definitively link the parameters of galaxies to the tidal features that form. The novel process of creating a large catalogue of interacting galaxies was detailed in Chapter \ref{chapter2}. By using newly developed data-access architecture with the newly developed Bayesian CNN, \texttt{Zoobot}, the largest catalogue of interacting galaxies to date was created. Along with this, we demonstrated how the new data-access architecture, ESA Datalabs, can be used to explore archival data volumes easily and efficiently. To make concrete links between galaxy evolution and interaction in the local volume, this catalogue was cross-matched to catalogues of ancillary data. This was done using the catalogues created from deep observations of the COSMOS field. This provided us with a flux limited sample of 4,181 and volume limited sample of 3,384, interacting galaxies to explore.

In Chapter \ref{chapter3} we conducted this exploration of our cross-matched sample in the context of interaction stage. We investigated whether conflicting theories about galaxy interaction were a result of not accounting for interaction stage based on observed morphology. We confirmed that interaction stage does have a significant impact on the underlying processes and enhancements that result from interaction. It was immediately found that the SFR increases dramatically through stage - from close pair to merger. This was demonstrated by measuring the distributions of stellar mass and SFR through stage and comparing them. We found that for distributions of stellar mass drawn from the same parent sample, the distribution of SFR changes. This was in the form of the disappearance of the red sequence, as our samples SFR was enhanced. We compared this with existing works which utilise the projected separation between systems as an approximation of the stage of interaction in the two systems. We found a complete disconnect with the projected separation and stage, noting that conflicting results can emerge from only looking at the projected separation without properly accounting for the interaction stage from the morphology. 

This degeneracy in the projected separation was particularly true of the change in the fraction of active galactic nuclei with stage, a topic the literature is particularly divided upon. We found that AGN fraction remained consistent with the field at 0.12 throughout the interaction, until coalescence where the fraction increased to 0.15. Breaking down our AGN counts with projected separation, we found two peaks in AGN count. Thus, this was not only evidence for AGN flickering, but also that there is some delay in the activation of the AGN which is difficult to account for when only using projected separation. While this methodology shows how we can use ancillary data to connect the underlying parameters of galaxies to interaction, and fundamental processes we attempted to conduct this in a more general way and to bring in linking to tidal feature formation. 

Chapter \ref{chapter4} saw the development of an algorithm to find the underlying parameters the galaxys' involved in interaction purely from their flux distribution. We combined a fast, efficient restricted numerical simulation with a MCMC methodology and Bayesian statistics and placed constraints on the underlying parameters of 51 synthetic images of interacting systems. While our uncertainties increased when applied to observational data, constraints were nonetheless able to be made upon the majority of the parameters. Constraints only failed on the orientation parameters. The creation of this MCMC algorithm paves the way to apply this to large interacting galaxy datasets such as the one described in Chapter \ref{chapter2}. However, the limitations in the runtime of the algorithm; taking approximately 15-20 hours per interacting galaxy, leads this to not be feasible. Therefore, methods of further improvement in efficiency of the underling numerical simulation must be explored. There is particular promise in this regard with the development of numerical simulations on GPUs, and the massive acceleration they provide to such projects.

We have created a large interacting galaxy dataset and demonstrated its capability in the context of how the progression of a galaxy changes the underlying properties of galaxies. We have introduced a method by which further examination of these systems could be explored, although further advances in its efficiency must be made for this approach to be viable. Potential methods, plus descriptions of future works in Chapters \ref{chapter2} and \ref{chapter3}, are discussed in the final subsection of this thesis.

\section{Future Work}
\subsection{Catalogues of Galaxy Morphology with Ancillary Data}
% Creating the largest catalogue of interacting galaxies has been done. Why not for all morphologies? Demonstrates the capability of ESA Datalabs. Mention this combination with Euclid, JWST and such.
\noindent Chapter \ref{chapter2} demonstrated that new data access architecture is now ready for us to conduct source classification at a scale rarely seen to this point. We can now directly access millions of sources across multiple filters, instruments and observatories to yield unprecedented sample sizes. These can be combined with novel machine learning algorithms requiring small training set sizes to fine-tune making them versatile and accurate across many different observatories. Since the publication of the paper underlaying Chapter \ref{chapter2}, the \texttt{Zoobot} algorithm has been updated many times. Not only is it now trained upon \textit{HST} data, but part of the representations of galaxies it learns is if a galaxy is interacting or not. By redoing the work conducted in Chapter \ref{chapter2}, one could classify galactic morphologies across the Galaxy Zoo workflow rather than just on an individual question.

The fundamental component of this which makes it possible is the ESA Datalabs platform. This platform has also been updated to provide access to many different observatories archives within it (such as \emph{JWST} and \emph{Euclid}) and also has access to much larger storage spaces and GPUs. Therefore, the entire work of Chapter \ref{chapter2} can be done on the platform with no requirement for data transfer (in our case, data had to be moved from ESA Datalabs to the Lancaster computer cluster for classification, taking the bulk of the project time). ESA Datalabs now allows us to conduct to a project similar to the one outlined in Chapter \ref{chapter2} with much greater efficiency. Not only could this classification be applied for interacting vs non-interacting, but we can also begin to consider creating larger catalogues of much broader morphology classifications. The recent Galaxy Zoo: DESI release contained 9.7 million galaxies with full morphology classification. Applying this to just the sources created in Chapter \ref{chapter2} would find 126 million morphology classifications, which is over a factor of 10 greater than previously achieved.

We can also take this further and not limit the opportunity here to only to only large catalogues of morphology classification. With the all-sky photometry that will be available from \emph{Euclid}, or survey scale photometry that will be available from ground based observatories like the Legacy Survey of Space and Time telescope, it will be possible to get broad-band photometry for millions of overlapping sources. By applying well-known astrophysical software such as \texttt{FAST}, \texttt{EAzY} or \texttt{LePhare}, it will be able to estimate many galactic parameters. These include the stellar masses, the photometric redshifts or, in some cases, the presence of AGN. By creating such large morphology catalogues, combined with this ancillary data, we will be able to robustly link different galaxy parameters to their physical morphologies. This can then be expanded out to include linking to the source environments across the sky. 

The development of algorithms like those proposed and tested in Chapter \ref{chapter4} will also provide excellent methods for constraining the underlying parameters of sources. While Chapter \ref{chapter4} focused on such an algorithm in the context of interacting galaxies, algorithms such as \texttt{GALFIT} are able to find morphological parameters purely from comparison to flux distributions and images. Even without further broad band photometry from other observatories, we will be able to explore the underlying parameter spaces of galaxies and link this to their morphologies using these data access architectures. 

\subsection{Constraining Interacting Galaxy Parameters}
% ALOT to talk about here. So, can discuss how GPUs are beginning to run numerical simulations with unprecedented efficiency. Talk about the rise in simulation based inference, and the easy transference of this pipeline to that. 
\noindent In Chapter \ref{chapter4}, we made direct links between the flux distribution of interacting galaxies and the parameters of those galaxies. We made constraints and revealed degeneracies across multiple parameters. However, the limitation of this approach was the computational expense, taking between 15-20 hours per system with the associated run time cost for this. For the large scale surveys or the catalogues such as those created in Chapter \ref{chapter2}, this efficiency is not enough. Making constraints on the entire sample selected in Chapter \ref{chapter2} would take between 37 and 50 years. However, there are multiple development routes that could be taken in future to boost computational efficiency. The first is to take full advantage of the next generation of GPUs. Many cases of accelerating numerical simulations have been written about and analysed, particularly from the GPU developer NVIDIA. These have demonstrated striking improvements in efficiency up to a factor of 3, especially in the context of numerical models of fluid dynamics \citep[recent examples include][]{Mantas2016, COSTA2021502}. Thus, reworking the code in this way would remove this limitation and have constraint being made in a matter of minutes: much more feasible for applying to large scale samples.

A second approach would be to move away from the direct method of MCMC and into utilising simulation based inference (SBI), often called likelihood free inference. Work has been conducted into SBI, and its massive increases in efficiency when constraining over large and complex parameter spaces \citep[for an excellent description of likelihood free inference, see][]{2021MNRAS.501..954J}. In this context, rather than running a MCMC and directly comparing simulation outputs to an observational image, a machine learning algorithm trained on simulation outputs is used to explore the parameter space. By running an initial set of simulations and feeding this into a rudimentary CNN, a low dimensional representative vector can be created of each image. This is then applied to the observational image. The CNN then approximates the posterior through parameter space, achieving constraints comparable to a direct MCMC exploration. However, while this approach has been very successful for small parameter spaces defining 1D cases, applying to cases of images - especially one defined by a 13D parameter space presented here - is in its infancy. 

Once this is evolved, however, it would succeed in significant improvements in the efficiency of our constraining algorithms. We could take advantage of the amortisation of SBI, and be able to apply our initially trained network and posterior distribution to many different systems instead of having to explore parameter space directly every time. This has the effect of front-loading the computational expense of our constraining process, running thousands to millions of simulations across parameter space initially to train the neural network and then being able to use the model in multiple cases, albeit with some caveats. Amortisation of the trained model across parameter space would be very sensitive to many different fundamental parts of our analysis process. For instance, changing the position of the secondary galaxy, the changing resolution of the images, the number of particles in the simulation, etc, all could be attempted. Changing any of these would mean amortisation is not valid, and the model would have to be retrained - re-introducing the problem of computational efficiency. 

However, if these caveats can be resolved, this would open the way for large-scale application of SBI and our direct inferencing methodology to large samples. These will be our next steps in the development of this algorithm, and to then make it widely accessible to the community, with the application of our sample of interacting galaxies from Chapter \ref{chapter2}. We will then be able to apply the process as conducted in Chapter \ref{chapter3}, and link the effects of interaction to further underlying parameters than the stellar mass, interaction stage and SFR.