\chapter{SUMMARY AND FUTURE WORK}
\section{SUMMARY}
\noindent This work has investigated the role of galaxy interaction with respect to galaxy evolution, shown a new and novel way of creating large interacting galaxy samples and presented the initial pilot of a pipeline to definitively link the parameters of galaxies to the tidal features that form and their underlying processes. The novel process of creating a large catalogue of interacting galaxies was detailed in chapter 2. By utilising newly developed data-access architecture with the newly developed Bayesian CNN \texttt{Zoobot} the largest catalogue of interacting galaxies to date was created. Along with this, we demonstrated how the new data-access architecture, ESA Datalabs, can be used to explore archival observations like never before. To make concrete links between galaxy evolution and interaction in the local volume, this catalogue was cross-matched to catalogues of ancillary data. This was done using the catalogues created from deep observations of the COSMOS field. This provided us with a sample of 4,135 interacting galaxies to explore.

In Chapter 3 we conducted this exploration of our cross-matched sample in the context of interaction stage. We investigated whether the often conflicting theories about galaxy interaction were, in fact, from not accounting for interaction stage based on observed morphology. We confirmed that interaction stage does have a significant impact on the underlying processes and enhancements that result from interaction. It was immediately found that the SFR increases dramatically through stage - from close pair to merger. This was demonstrated by measuring the distributions of stellar mass and SFR through stage and comparing them. We found that for distributions of identical mass, the distribution of SFR changes completely. This is in the form of the disappearence of the red sequence, as our samples SFR was enhanced. We compared this with existing works which utilise the projected separation between systems as an approximation of the stage of interaction in the two systems. We find a complete disconnect with the projected separation and stage, noting that conflicting results can emerge from only looking at the projected separation without properly accounting for the interaction stage from the morphology. 

This degeneracy in the projected separation was particularily true of the change in the fraction of active galactic nuclei with stage, a topic the literature is particularily divided upon. Two modes of activation were found, one at stage 2 and one at stage 3. Thus, this was not only evidence for AGN flickering, but also that there is some delay in the activation of the AGN which is difficult to account for when only using projected separation. While this methodology shows how we can use ancillary data to connect the underlying parameters of galaxies to interaction, and fundamental processes we attempted to conduct this in a more general way and to bring in linking to tidal feature formation. 

Chapter 4 saw the development of an algorithm to find the underlying parameters the galaxys' involved in interaction purely from their flux distribution. We combine a fast, efficient restricted numerical simulation with a MCMC methodology and Bayesian statistics and put constraints on the underlying parameters of 38 simulated interacting systems. While our uncertainties increased when applied to observational data, constraints were nonetheless able to be made upon them. The creation of this algorithm, APySPAM, paves the way to apply this to large interacting galaxy datasets like the one created in chapter 2. However, the limitations in the runtime of the algorithm; taking approximately 15-20 hours per interacting galaxy, leads this to not be feasible. Thus, methods of further improvement in efficiency of the underling numerical simulation must be explored. There is particular promise in this regard with the development of numerical simulations on GPUs, and the massive acceleration they provide to such projects.

Thus, I have created a large interacting galaxy dataset and demonstrated it's capability in the context of how the progression of a galaxy changes the underlying properties of galaxies. We have introduced a method by which further examination of these systems could be explored, although further advances in its efficiency must be made for this approach to be viable. Potential methods, plus descriptions of future works in chapter 2 and 3 are discussed in the final subsection of this thesis.

\section{FUTURE WORK}
\subsection{Catalogues of Galaxy Morphology with Ancillary Data}
% Creating the largest catalogue of interacting galaxies has been done. Why not for all morphologies? Demonstrates the capability of ESA Datalabs. Mention this combination with Euclid, JWST and such.
\noindent Chapter 2 demonstrated that new data access architecture is now ready for us to conduct source classification at a scale rarely seen to this point. We can now directly access millions of sources across multiple filters, instruments and observatories to yield unprecedented sample sizes. These can be combined with novel machine learning algorithms requiring small training set sizes to fine-tune, and thus being versitile and accurate across many different observatories. Since the publication of the paper underlaying chapter 2, the \texttt{Zoobot} algorithm has been updated many times. Not only is it now trained upon \textit{HST} data, but part of the representations of galaxies it learns is if a galaxy is interacting or not. Thus, by redoing the work conducted in chapter 2, one could classify galactic morphologies across the Galaxy Zoo workflow rather than just an individual question.

The fundamental component of this which makes it possible is the ESA Datalabs platform. This platform has also been heavily updated to, not only, provide access to many different observatory's archives within it (such as JWST and Euclid) but has access to much larger storage spaces and GPUs. Thus, the entire work of chapter 2 can be done on the platform with no requirement for data transfer (in our case, data had to be moved from ESA Datalabs to the Lancaster computer cluster for classification: taking the bulk of the project time). Thus, ESA Datalabs now allows us to conduct a project similar to Chapter 2 with much greater efficiency. Not only could this classification be applied for interacting vs non-interacting, but we can also begin to consider creating larger catalogues of much broader morpholgoy classifications. The recent Galaxy Zoo: DESI release contained 9.7 million galaxies with full morphology classification. Applying this to just the sources created in Chapter 2 would find 126 million morphology classifications - over a factor of 10 greater.

We can also take this further, and not just create large catalogues of morphology classification. With the all-sky photometry that will be available from Euclid, or survey scale photometry that will be available from ground based observatories like the Legacy Survey of Space and Time telescope, it will be possible to get broad-band photometry for millions of overlapping sources. Thus, by applying well-known astrophysical software such as FAST, EAzY or LePhare, it will be able to estimate many galactic parameters. These include the stellar masses, the photometric redshifts or, in some cases, the precense of AGN. By creating such large morphology catalogues, combined with this ancillary data, we will be able to robustly link different galaxy parameters to their physical morphologies. This can then be expanded out to include linking to the source environments across the sky. 

The development of algorithms like those proposed and tested in Chapter 4 will also provide excellent methods for constraining the underlying parameters of sources. While Chapter 4 focused on such an algorithm in the context of interacting galaxies, algorithms such as GALFIT are able to find morphological parameters purely from comparison to flux distributions and images. Thus, even without further broad band photometry from other observatories, we will be able to explore the underlying parameter spaces of galaxy's and link this to their morphologies using these data access architectures. 

\subsection{Constraining Interacting Galaxy Parameters}
% ALOT to talk about here. So, can discuss how GPUs are beginning to run numerical simulations with unprecedented efficiency. Talk about the rise in simulation based inference, and the easy transference of this pipeline to that. 
\noindent Finally, there is the question of relating the underlying parameters of interaction to the tidal features that form. In chapter 4, we attempted to link these directly by mapping the flux distribution of interacting galaxies to that of simulated interactions. In these, we were able to find constraints and reveal degeneracies across multiple parameters. However, the limitation of this approach was the computational expense, taking approximately 15 hours per system. For the large scale surveys soon to come online and catalogues such as those created in Chapter 2, efficiency is not enough. However, there are multiple development routes that this could go down. The first is to completely update the code to be accessible on a GPU. Development of numerical codes on GPUs remains, at time of writing, in its infancy. However, there have been striking results in improvements in efficiency up to a factor of 3, especially in the context of numerical models of fluid dynamics \citep{There tonnes of papers of GPUs on fluid codes}. Thus, reworking the code in this way would remove this limitation and have constraint being made in a matter of minutes: much more feasible for applying to large scale samples.

A second approach would be to move away from the direct, brute force method and into utilising simulation based inference (SBI). Much work has been conducted into SBI, and its massive increases in efficiency when constraining over large and complex parameter spaces \citep[for an excellent describtion of likelihood free inference, see][]{2021MNRAS.501..954J}. In this context, rather than running an MCMC and directly comparing simulation outputs to an observational image, a machine learning algorithm is used for the comparison. By running an initial set of simulations and feeding this into a rudimentary CNN, a low dimensional representative vector can be created of each image. This is then applied to the observational image. The CNN then approximates the posterior through parameter space, achieving excellent accuracy in the cited cases. However, while this approach has been very successful for small parameter spaces, and approximating different distributions work is still requried in image recognition - especially across the 14 dimensions of interacting parameter space presented here. 

In the context of large samples of interacting galaxies, SBI massively improves the efficiency of our constraining algorithms. Rather than running thousands of simulations on every single system to constrain it, we can front-load the computational expense of running the simulations to train the CNN and then apply the CNN to the entire interacting galaxy sample. This does have some caveats, however. The resolution of each image would have to be identical, and the redshift of the system would have to be accounted for. However, once further work SBI with images is conducted, it will become a valid approach. 